{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8mDMWSyMEK5rkqqafrZ3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harinijs03/2023103549_SDC_assignment/blob/main/Medicalconsultant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "H10b-THlGI09",
        "outputId": "8677ee4f-0a29-4515-a79b-180b5d8fd864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://231800f6e153790652.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://231800f6e153790652.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "!pip install -q langchain pypdf faiss-cpu gradio sentence-transformers transformers\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set up text processing\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Initialize embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Initialize LLM (Flan-T5 for efficient text processing)\n",
        "summarizer = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_length=512,\n",
        "    truncation=True\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=summarizer)\n",
        "\n",
        "def analyze_document(file, question):\n",
        "    \"\"\"Analyze the uploaded document and answer questions\"\"\"\n",
        "    if not file:\n",
        "        return \"‚ö†Ô∏è Please upload a PDF document first\"\n",
        "    if not question:\n",
        "        return \"‚ö†Ô∏è Please enter a question about the document\"\n",
        "\n",
        "    try:\n",
        "        # Save uploaded file temporarily\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        file_path = os.path.join(temp_dir, \"uploaded_file.pdf\")\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(file)\n",
        "\n",
        "        # Load and split the PDF\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        pages = loader.load()\n",
        "        docs = text_splitter.split_documents(pages)\n",
        "\n",
        "        # Create searchable vector database\n",
        "        db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "        # Create question-answering chain\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=db.as_retriever(search_kwargs={\"k\": 3})\n",
        "        )\n",
        "\n",
        "        # Get answer with page references\n",
        "        result = qa_chain({\"query\": question})\n",
        "        answer = result[\"result\"]\n",
        "\n",
        "        # Add page references if available\n",
        "        if 'source_documents' in result:\n",
        "            pages = {str(doc.metadata.get('page', '?')) for doc in result['source_documents']}\n",
        "            answer += f\"\\n\\n(Found in pages: {', '.join(sorted(pages))})\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error processing document: {str(e)}\"\n",
        "    finally:\n",
        "        # Clean up temporary files\n",
        "        if 'temp_dir' in locals():\n",
        "            for root, dirs, files in os.walk(temp_dir, topdown=False):\n",
        "                for name in files:\n",
        "                    os.remove(os.path.join(root, name))\n",
        "                for name in dirs:\n",
        "                    os.rmdir(os.path.join(root, name))\n",
        "            os.rmdir(temp_dir)\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Medical Document Consultant\") as app:\n",
        "    gr.Markdown(\"## üìÑ Medical Document Consultant\")\n",
        "    gr.Markdown(\"Upload a medical document and ask questions about its content\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(\n",
        "                label=\"Upload Medical PDF\",\n",
        "                type=\"binary\",\n",
        "                file_types=[\".pdf\"]\n",
        "            )\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"What would you like to know about this document?\"\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Analyze\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(\n",
        "                label=\"Answer\",\n",
        "                interactive=False,\n",
        "                lines=10\n",
        "            )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=analyze_document,\n",
        "        inputs=[file_input, question_input],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "app.launch(share=True)"
      ]
    }
  ]
}