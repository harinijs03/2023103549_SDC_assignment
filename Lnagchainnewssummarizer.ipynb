{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAXnnuX6no45jkgjYgb0+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harinijs03/2023103549_SDC_assignment/blob/main/Lnagchainnewssummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuMmeRZjezy7"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required packages (Colab only)\n",
        "!pip install -q langchain google-generativeai\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from typing import List, Optional, ClassVar\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Step 3: Set up Gemini API key (as you allowed)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBzeETE_HhcD7zsf0bYY4uuoiiot6jjfvw\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Step 4: Define custom LangChain-compatible wrapper for Gemini 1.5 Flash\n",
        "class GeminiFlashLLM(LLM):\n",
        "    model_name: ClassVar[str] = \"gemini-1.5-flash\"\n",
        "    temperature: ClassVar[float] = 0.3\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        try:\n",
        "            model = genai.GenerativeModel(self.model_name)\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemini-1.5-flash\"\n",
        "\n",
        "# Step 5: Create a prompt template\n",
        "template = \"\"\"\n",
        "Summarize the following news article into 3–4 concise bullet points:\n",
        "\n",
        "{text}\n",
        "\n",
        "Summary:\n",
        "-\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Step 6: Create the LLM and build the chain using modern LangChain syntax\n",
        "llm = GeminiFlashLLM()\n",
        "chain = prompt | llm  # modern Runnable-style chain\n",
        "\n",
        "# Step 7: Provide a sample article\n",
        "article = \"\"\"\n",
        "Scientists have announced a breakthrough in nuclear fusion research at the National Ignition Facility (NIF)\n",
        "in the United States. For the second time, they have achieved net energy gain in a fusion reaction,\n",
        "meaning the energy produced by the fusion exceeded the energy used to initiate it. This milestone\n",
        "is considered a significant step towards the long-term goal of developing clean and sustainable energy\n",
        "through nuclear fusion. However, experts caution that there are still considerable engineering and\n",
        "scientific challenges to overcome before fusion power becomes a practical reality.\n",
        "\"\"\"\n",
        "\n",
        "# Step 8: Run the summarization chain with `.invoke()`\n",
        "summary = chain.invoke({\"text\": article})\n",
        "\n",
        "# Step 9: Print the result\n",
        "print(\"📄 Summary Output:\\n\", summary)"
      ]
    }
  ]
}